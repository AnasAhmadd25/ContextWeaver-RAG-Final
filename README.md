# ContextWeaver: Advanced RAG System

ContextWeaver is a Retrieval-Augmented Generation (RAG) system designed to provide accurate, grounded responses by combining state-of-the-art embedding models with large language models.

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ backend/                # FastAPI Backend Application
â”‚   â”œâ”€â”€ core/               # Core RAG Logic and Database interactions
â”‚   â”‚   â”œâ”€â”€ database.py     # Qdrant Vector Database integration
â”‚   â”‚   â”œâ”€â”€ models.py       # Embedding model (BGE) initialization
â”‚   â”‚   â””â”€â”€ rag.py          # RAG pipeline and Gemini LLM integration
â”‚   â”œâ”€â”€ data/               # Persistent storage for Qdrant (local mode)
â”‚   â”œâ”€â”€ main.py             # FastAPI entry point and API routes
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ .env                # Environment variables (API Keys, etc.)
â”œâ”€â”€ frontend/               # Frontend Application
â”‚   â””â”€â”€ index.html          # Single-page interface with Teal theme
â””â”€â”€ embedding_comparison.ipynb # Research notebook for embedding model evaluation
```

## ğŸš€ Main Components

### 1. Backend (FastAPI)
The backend serves as the orchestration layer, handling requests from the frontend and coordinating between the vector database and the LLM.
- **`query`**: Processes user questions, retrieves context, and generates answers.
- **`upload`**: Handles PDF and Text file uploads, extracts text, and indexes them into the vector DB.
- **`feedback`**: Captures user feedback for future self-learning optimizations.
- **`seed`**: Populates the database with initial mock data for testing.

### 2. Core Logic (`backend/core`)
- **`database.py`**: Utilizes **Qdrant** for vector storage. It manages collection creation and handles "upsert" operations for document embeddings.
- **`models.py`**: Configures the **BAAI/bge-small-en-v1.5** embedding model via `sentence-transformers`. It includes specific prompting logic to optimize retrieval accuracy.
- **`rag.py`**: Implements the RAG workflow. It retrieves relevant document context from Qdrant and constructs a grounded prompt for the **Google Gemini** model (Gemini 2.5 Flash / 1.5 Flash).

### 3. Frontend
A modern, responsive single-page application built with a premium **Teal theme**. It features:
- Interactive chat interface for RAG queries.
- File upload functionality for knowledge base expansion.
- Real-time feedback submission (Like/Dislike).
- Glassmorphism design elements and smooth animations.

### 4. Embedding Comparison Notebook
A Jupyter notebook used during the development phase to evaluate different embedding models (`LaBSE`, `all-MiniLM-L6-v2`, `BGE`, etc.) against a custom evaluation dataset to ensure the best retrieval performance.

## ğŸ› ï¸ Setup & Installation

1. **Backend**:
   ```bash
   cd backend
   pip install -r requirements.txt
   # Add your GEMINI_API_KEY to .env
   python main.py
   ```

2. **Frontend**:
   Simply open `frontend/index.html` in a browser or serve it using a local live server.

## ğŸ§  Technologies Used
- **Language**: Python (Backend), Javascript/HTML/CSS (Frontend)
- **Framework**: FastAPI
- **Vector DB**: Qdrant
- **Embeddings**: BGE (HuggingFace)
- **LLM**: Google Gemini
- **UI**: Vanilla CSS (Custom Design System)
